{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SqueezeNet\n",
    "\n",
    "Described here: https://towardsdatascience.com/review-squeezenet-image-classification-e7414825581a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Concatenate, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import audiomentations as AA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from pysndfx import AudioEffectsChain\n",
    "import math\n",
    "import python_speech_features\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://xeno-canto.org/api/2/recordings?query=cnt:%22United%20States%22\"  # Replace with the actual URL\n",
    "\n",
    "# Function to process each recording object\n",
    "def process_recording(recording):\n",
    "    return {\n",
    "        'genus': recording['gen'],\n",
    "        'species': recording['sp'],\n",
    "        'name': recording['en'],\n",
    "        'file': recording['file']\n",
    "    }\n",
    "\n",
    "def fetch_data():\n",
    "    page = 4\n",
    "    results = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, params={\"page\": page})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            num_pages = int(data[\"numPages\"])\n",
    "            recordings = data[\"recordings\"]\n",
    "\n",
    "            # Process each recording object\n",
    "            results.extend([{\n",
    "                'genus': recording['gen'],\n",
    "                'species': recording['sp'],\n",
    "                'name': recording['en'],\n",
    "                'file': recording['file'],\n",
    "                'filename': recording['file-name']\n",
    "            } for recording in recordings])\n",
    "\n",
    "            if page == num_pages:\n",
    "                break  # Exit the loop if all pages have been fetched\n",
    "\n",
    "            page += 1  # Increment page for the next request\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            break  # Stop fetching data in case of an error\n",
    "    return results\n",
    "\n",
    "call_recordings = fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(results, subdirectory):\n",
    "    os.makedirs(subdirectory, exist_ok=True)  # Create subdirectory if it doesn't exist\n",
    "\n",
    "    for result in results:\n",
    "        file_url = result['file']\n",
    "        filename = result['filename']\n",
    "        file_path = os.path.join(subdirectory, filename)  # Construct the file path\n",
    "\n",
    "        file = Path(file_path)\n",
    "        if file.is_file():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Download and save the file\n",
    "            response = requests.get(file_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                    print(f\"File '{filename}' downloaded successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to download file '{filename}'. Status code: {response.status_code}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Exception while attempting to download file '{filename}'. Exception: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save metadata as a separate JSON file\n",
    "        metadata_filename = f\"{filename}.json\"\n",
    "        metadata_path = os.path.join(subdirectory, metadata_filename)\n",
    "        with open(metadata_path, 'w') as metadata_file:\n",
    "            json.dump(result, metadata_file, indent=4)\n",
    "            print(f\"Metadata file '{metadata_filename}' saved successfully.\")\n",
    "\n",
    "\n",
    "download_files(call_recordings, 'calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filters adapted from https://github.com/dodiku/noise_reduction\n",
    "\n",
    "'''------------------------------------\n",
    "NOISE REDUCTION USING POWER:\n",
    "    receives an audio matrix,\n",
    "    returns the matrix after gain reduction on noise\n",
    "------------------------------------'''\n",
    "def reduce_noise_power(y, sr):\n",
    "\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "    threshold_h = round(np.median(cent))*1.5\n",
    "    threshold_l = round(np.median(cent))*0.1\n",
    "\n",
    "    less_noise = AudioEffectsChain().lowshelf(gain=-30.0, frequency=threshold_l, slope=0.8).highshelf(gain=-12.0, frequency=threshold_h, slope=0.5)#.limiter(gain=6.0)\n",
    "    y_clean = less_noise(y)\n",
    "\n",
    "    return y_clean\n",
    "\n",
    "\n",
    "'''------------------------------------\n",
    "NOISE REDUCTION USING CENTROID ANALYSIS:\n",
    "    receives an audio matrix,\n",
    "    returns the matrix after gain reduction on noise\n",
    "------------------------------------'''\n",
    "\n",
    "def reduce_noise_centroid_s(y, sr):\n",
    "\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "    cent = cent[cent != 0]\n",
    "\n",
    "    threshold_h = np.max(cent)\n",
    "    threshold_l = np.min(cent)\n",
    "\n",
    "\n",
    "    #less_noise = AudioEffectsChain().lowshelf(gain=-12.0, frequency=threshold_l, slope=0.5).highshelf(gain=-12.0, frequency=threshold_h, slope=0.5).limiter(gain=6.0)\n",
    "\n",
    "    less_noise = AA.Compose([\n",
    "        AA.LowShelfFilter(min_center_freq=0, max_center_freq=threshold_l, max_gain_db=-12.0, min_q=0.5, max_q=0.5, p=1),\n",
    "        AA.HighShelfFilter(min_center_freq=threshold_h, max_center_freq=2 * sr , max_gain_db=-12.0, min_q=0.5, max_q=0.5, p=1),\n",
    "        AA.Limiter(max_threshold_db=6.0, p=1)\n",
    "    ])\n",
    "\n",
    "    #y_speach_boosted = speech_booster(y, sample_rate=sr)\n",
    "\n",
    "    y_cleaned = less_noise(y, sample_rate=sr)\n",
    "\n",
    "    return y_cleaned\n",
    "\n",
    "def reduce_noise_centroid_mb(y, sr):\n",
    "\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "    threshold_h = np.max(cent)\n",
    "    threshold_l = np.min(cent)\n",
    "\n",
    "    less_noise = AudioEffectsChain().lowshelf(gain=-30.0, frequency=threshold_l, slope=0.5).highshelf(gain=-30.0, frequency=threshold_h, slope=0.5).limiter(gain=10.0)\n",
    "    # less_noise = AudioEffectsChain().lowpass(frequency=threshold_h).highpass(frequency=threshold_l)\n",
    "    y_cleaned = less_noise(y)\n",
    "\n",
    "\n",
    "    cent_cleaned = librosa.feature.spectral_centroid(y=y_cleaned, sr=sr)\n",
    "    columns, rows = cent_cleaned.shape\n",
    "    boost_h = math.floor(rows/3*2)\n",
    "    boost_l = math.floor(rows/6)\n",
    "    boost = math.floor(rows/3)\n",
    "\n",
    "    # boost_bass = AudioEffectsChain().lowshelf(gain=20.0, frequency=boost, slope=0.8)\n",
    "    boost_bass = AudioEffectsChain().lowshelf(gain=16.0, frequency=boost_h, slope=0.5)#.lowshelf(gain=-20.0, frequency=boost_l, slope=0.8)\n",
    "    y_clean_boosted = boost_bass(y_cleaned)\n",
    "\n",
    "    return y_clean_boosted\n",
    "\n",
    "\n",
    "'''------------------------------------\n",
    "NOISE REDUCTION USING MFCC:\n",
    "    receives an audio matrix,\n",
    "    returns the matrix after gain reduction on noise\n",
    "------------------------------------'''\n",
    "def reduce_noise_mfcc_down(y, sr):\n",
    "\n",
    "    hop_length = 512\n",
    "\n",
    "    ## librosa\n",
    "    # mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "    # librosa.mel_to_hz(mfcc)\n",
    "\n",
    "    ## mfcc\n",
    "    mfcc = python_speech_features.base.mfcc(y)\n",
    "    mfcc = python_speech_features.base.logfbank(y)\n",
    "    mfcc = python_speech_features.base.lifter(mfcc)\n",
    "\n",
    "    sum_of_squares = []\n",
    "    index = -1\n",
    "    for r in mfcc:\n",
    "        sum_of_squares.append(0)\n",
    "        index = index + 1\n",
    "        for n in r:\n",
    "            sum_of_squares[index] = sum_of_squares[index] + n**2\n",
    "\n",
    "    strongest_frame = sum_of_squares.index(max(sum_of_squares))\n",
    "    hz = python_speech_features.base.mel2hz(mfcc[strongest_frame])\n",
    "\n",
    "    max_hz = max(hz)\n",
    "    min_hz = min(hz)\n",
    "\n",
    "    #speech_booster = AudioEffectsChain().highshelf(frequency=min_hz*(-1)*1.2, gain=-12.0, slope=0.6).limiter(gain=8.0)\n",
    "    \n",
    "    speech_booster = AA.Compose([\n",
    "        AA.HighShelfFilter(min_center_freq=min_hz*(-1)*1.2, max_gain_db=-12.0, min_q=0.6, max_q=0.6),\n",
    "        AA.Limiter(max_threshold_db=8.0)\n",
    "    ])\n",
    "\n",
    "    y_speach_boosted = speech_booster(y, sample_rate=sr)\n",
    "\n",
    "    return (y_speach_boosted)\n",
    "\n",
    "def reduce_noise_mfcc_up(y, sr):\n",
    "\n",
    "    hop_length = 512\n",
    "\n",
    "    ## librosa\n",
    "    # mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "    # librosa.mel_to_hz(mfcc)\n",
    "\n",
    "    ## mfcc\n",
    "    mfcc = python_speech_features.base.mfcc(y)\n",
    "    mfcc = python_speech_features.base.logfbank(y)\n",
    "    mfcc = python_speech_features.base.lifter(mfcc)\n",
    "\n",
    "    sum_of_squares = []\n",
    "    index = -1\n",
    "    for r in mfcc:\n",
    "        sum_of_squares.append(0)\n",
    "        index = index + 1\n",
    "        for n in r:\n",
    "            sum_of_squares[index] = sum_of_squares[index] + n**2\n",
    "\n",
    "    strongest_frame = sum_of_squares.index(max(sum_of_squares))\n",
    "    hz = python_speech_features.base.mel2hz(mfcc[strongest_frame])\n",
    "\n",
    "    max_hz = max(hz)\n",
    "    min_hz = min(hz)\n",
    "\n",
    "    #speech_booster = AudioEffectsChain().lowshelf(frequency=min_hz*(-1), gain=12.0, slope=0.5)#.highshelf(frequency=min_hz*(-1)*1.2, gain=-12.0, slope=0.5)#.limiter(gain=8.0)\n",
    "    \n",
    "    speech_booster = AA.Compose([\n",
    "        AA.LowShelfFilter(min_center_freq=min_hz*(-1), max_gain_db=12.0, min_q=0.5, max_q=0.5)\n",
    "    ])\n",
    "\n",
    "    y_speach_boosted = speech_booster(y, sample_rate=sr)\n",
    "\n",
    "    return (y_speach_boosted)\n",
    "\n",
    "'''------------------------------------\n",
    "NOISE REDUCTION USING MEDIAN:\n",
    "    receives an audio matrix,\n",
    "    returns the matrix after gain reduction on noise\n",
    "------------------------------------'''\n",
    "\n",
    "def reduce_noise_median(y, sr):\n",
    "    y = sp.signal.medfilt(y,3)\n",
    "    return (y)\n",
    "\n",
    "def trim_silence(y):\n",
    "    y_trimmed, index = librosa.effects.trim(y, top_db=20, frame_length=2, hop_length=500)\n",
    "    trimmed_length = librosa.get_duration(y) - librosa.get_duration(y_trimmed)\n",
    "\n",
    "    return y_trimmed, trimmed_length\n",
    "\n",
    "def limiter(y, sr):\n",
    "    limiter = AA.Compose([\n",
    "        AA.Limiter()\n",
    "    ])\n",
    "\n",
    "    y_limited = limiter(y, sample_rate=sr)\n",
    "\n",
    "    return y_limited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunks(y, sr, chunk_size_s=10):\n",
    "    chunk_samples = chunk_size_s * sr  # Number of samples in each chunk\n",
    "    num_samples = len(y)  # Total number of samples\n",
    "    return [y[i : i + chunk_samples] for i in range(0, num_samples, chunk_samples)]\n",
    "\n",
    "\n",
    "def load_data(directory, test_size=0.2, random_state=42, max_count=math.inf):\n",
    "    labels = []\n",
    "\n",
    "    target_size = (224, 224)\n",
    "    count = 0\n",
    "\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    # Load and process audio files into MFCCs with data augmentation \n",
    "    mfccs = []\n",
    "\n",
    "    # Load filenames and labels from metadata JSON files\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(directory, filename)) as metadata_file:\n",
    "                metadata = json.load(metadata_file)\n",
    "                audio_path = os.path.join(directory, metadata['filename'])\n",
    "                y, sr = librosa.load(audio_path)\n",
    "\n",
    "                chunks = generate_chunks(y, sr)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    \n",
    "                    # clean up the noise\n",
    "                    augmented_y = limiter(chunk, sr)\n",
    "\n",
    "                    # Compute MFCCs\n",
    "                    mfcc =librosa.power_to_db(librosa.feature.melspectrogram(\n",
    "                        np.float32(augmented_y), sr=sr, n_fft=2048, hop_length=512, n_mels=target_size[0]), ref=np.max)\n",
    "                    mfcc = Image.fromarray(mfcc).resize(target_size)\n",
    "                    mfccs.append(mfcc)\n",
    "                    label = f\"{metadata['genus']} {metadata['species']}\"\n",
    "                    labels.append(label)\n",
    "                    class_counts[label] += 1\n",
    "\n",
    "                if count > max_count:\n",
    "                    break\n",
    "                count += 1\n",
    "\n",
    "    mask = np.vectorize(class_counts.get)(labels) > 1\n",
    "    labels = np.array(labels)[mask]\n",
    "    mfccs = np.array(mfccs)[mask]\n",
    "\n",
    "\n",
    "    # Encode labels to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    mfccs = np.stack(mfccs)\n",
    "    # Split the data into training and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(mfccs, encoded_labels, test_size=test_size, random_state=random_state, stratify=encoded_labels)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, label_encoder\n",
    "\n",
    "X_train, X_test, y_train, y_test, label_encoder = load_data('calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test2.shape)\n",
    "print(y_test4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the figure\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "ax1.imshow(np.swapaxes(X_train[0], 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
    "ax1.set_title(label_encoder.inverse_transform([y_train[0]])[0], {'fontsize':20, 'fontweight':'bold'})\n",
    "ax1.set_ylim(ax1.get_ylim()[::-1])\n",
    "ax2.imshow(np.swapaxes(X_train[1], 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
    "ax2.set_title(label_encoder.inverse_transform([y_train[1]])[0], {'fontsize':20, 'fontweight':'bold'})\n",
    "ax2.set_ylim(ax2.get_ylim()[::-1])\n",
    "ax3.imshow(np.swapaxes(X_train[2], 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
    "ax3.set_title(label_encoder.inverse_transform([y_train[2]])[0], {'fontsize':20, 'fontweight':'bold'})\n",
    "ax3.set_ylim(ax3.get_ylim()[::-1])\n",
    "ax4.imshow(np.swapaxes(X_train[3], 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
    "ax4.set_title(label_encoder.inverse_transform([y_train[3]])[0], {'fontsize':20, 'fontweight':'bold'})\n",
    "ax4.set_ylim(ax4.get_ylim()[::-1])\n",
    "fig.set_size_inches(18,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra dimension to the spectrogram data\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Add an extra dimension to the spectrogram data\n",
    "y_train = np.expand_dims(y_train, axis=-1)\n",
    "y_test = np.expand_dims(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    f_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    # Squeeze\n",
    "    x = Conv2D(squeeze, (1, 1), padding='valid', name=f_id + 'squeeze1x1')(x)\n",
    "    x = Activation('relu', name=f_id + 'relu_squeeze1x1')(x)\n",
    "\n",
    "    # Expand\n",
    "    left = Conv2D(expand, (1, 1), padding='valid', name=f_id + 'expand1x1')(x)\n",
    "    left = Activation('relu', name=f_id + 'relu_expand1x1')(left)\n",
    "\n",
    "    right = Conv2D(expand, (3, 3), padding='same', name=f_id + 'expand3x3')(x)\n",
    "    right = Activation('relu', name=f_id + 'relu_expand3x3')(right)\n",
    "\n",
    "    x = Concatenate(axis=-1, name=f_id + 'concat')([left, right])\n",
    "    return x\n",
    "\n",
    "def SqueezeNet(input_shape=(224, 224, 3), classes=1000):\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "\n",
    "    x = Conv2D(classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = SqueezeNet(input_shape=X_train[0].shape, classes=len(label_encoder.classes_))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, save_path, X_train):\n",
    "\n",
    "    def representative_dataset_gen():\n",
    "            for i in range(100):\n",
    "                yield [X_train[i:i + 1]]\n",
    "\n",
    "    # Convert the SavedModel to TensorFlow Lite format\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.representative_dataset = representative_dataset_gen\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the quantized model\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    return tflite_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm = quantize_model(model, \"test.tflite\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tflite_model_size(tflite_model):\n",
    "    tflite_model_file = Path(\"temp.tflite\")\n",
    "    with tflite_model_file.open(\"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    size = tflite_model_file.stat().st_size\n",
    "    tflite_model_file.unlink()\n",
    "    return size\n",
    "\n",
    "#print(get_tflite_model_size(model))\n",
    "print(get_tflite_model_size(qm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE595-lab3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
